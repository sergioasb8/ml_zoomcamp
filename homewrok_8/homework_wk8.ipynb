{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Info\n",
    "\n",
    "Notebook with all the code needed to solve the homework for the week number 8 of the machine learning zoomcamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instal the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting the data\n",
    "\n",
    "In this homework, we'll build a model for predicting if we have an image of a bee or a wasp. For this, we will use the \"Bee or Wasp?\" dataset that was obtained from Kaggle and slightly rebuilt.\n",
    "\n",
    "You can download the dataset for this homework from here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/SVizor42/ML_Zoomcamp/releases/download/bee-wasp-data/data.zip\n",
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The dataset contains around 2500 images of bees and around 2100 images of wasps.\n",
    "\n",
    "The dataset contains separate folders for training and test sets.\n",
    "\n",
    "### Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "* The shape for input should be (150, 150, 3)\n",
    "* Next, create a convolutional layer (Conv2D):\n",
    "    * Use 32 filters\n",
    "    * Kernel size should be (3, 3) (that's the size of the filter)\n",
    "    * Use 'relu' as activation\n",
    "* Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "    * Set the pooling size to (2, 2)\n",
    "* Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "* Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "* Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "    * The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use SGD with the following parameters:\n",
    "\n",
    "* SGD(lr=0.002, momentum=0.8)\n",
    "\n",
    "For clarification about kernel size and max pooling, check Office Hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([    \n",
    "    keras.Input(shape=(150, 150, 3)),\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\"), \n",
    "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=64, activation=\"relu\"),\n",
    "    keras.layers.Dense(units=1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(\n",
    "    learning_rate=0.002, \n",
    "    momentum=0.8\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "* mean squared error\n",
    "* binary crossentropy\n",
    "* categorical crossentropy\n",
    "* cosine similarity\n",
    "\n",
    "\n",
    "Answer = binary crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "What's the number of parameters in the convolutional layer of our model? You can use the summary method for that.\n",
    "\n",
    "* 1\n",
    "* 65\n",
    "* 896\n",
    "* 11214912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 175232)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11215873 (42.79 MB)\n",
      "Trainable params: 11215873 (42.79 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer = 896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and test sets:\n",
    "\n",
    "```python\n",
    "    ImageDataGenerator(rescale=1./255)\n",
    "```\n",
    "\n",
    "* We don't need to do any additional pre-processing for the images.\n",
    "* When reading the data from train/test directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "* Use batch_size=20\n",
    "* Use shuffle=True for both training and test sets.\n",
    "\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "```python\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        validation_data=test_generator\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n",
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "val_gen   = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    \"./data/train/\", \n",
    "    target_size=(150, 150), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    \"./data/test/\", \n",
    "    target_size=(150, 150), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 11s 61ms/step - loss: 0.6775 - accuracy: 0.5611 - val_loss: 0.6352 - val_accuracy: 0.6155\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 10s 55ms/step - loss: 0.6262 - accuracy: 0.6435 - val_loss: 0.6001 - val_accuracy: 0.6296\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.5721 - accuracy: 0.7215 - val_loss: 0.5575 - val_accuracy: 0.7004\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.5347 - accuracy: 0.7449 - val_loss: 0.5474 - val_accuracy: 0.7200\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.5115 - accuracy: 0.7533 - val_loss: 0.5677 - val_accuracy: 0.6950\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 11s 60ms/step - loss: 0.4928 - accuracy: 0.7735 - val_loss: 0.5327 - val_accuracy: 0.7505\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.4657 - accuracy: 0.7955 - val_loss: 0.5157 - val_accuracy: 0.7614\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 11s 57ms/step - loss: 0.4360 - accuracy: 0.8104 - val_loss: 0.5262 - val_accuracy: 0.7538\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.4021 - accuracy: 0.8330 - val_loss: 0.5443 - val_accuracy: 0.7331\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 11s 58ms/step - loss: 0.3689 - accuracy: 0.8488 - val_loss: 0.5213 - val_accuracy: 0.7593\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "What is the median of training accuracy for all the epochs for this model?\n",
    "\n",
    "* 0.20\n",
    "* 0.40\n",
    "* 0.60\n",
    "* 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7633940875530243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(hist['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.763"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "What is the standard deviation of training loss for all the epochs for this model?\n",
    "\n",
    "* 0.031\n",
    "* 0.061\n",
    "* 0.091\n",
    "* 0.131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09234823155019067"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(hist[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.092"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "* rotation_range=50,\n",
    "* width_shift_range=0.1,\n",
    "* height_shift_range=0.1,\n",
    "* zoom_range=0.1,\n",
    "* horizontal_flip=True,\n",
    "* fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3677 images belonging to 2 classes.\n",
      "Found 918 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    \"./data/train/\", \n",
    "    target_size=(150, 150), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    \"./data/test/\", \n",
    "    target_size=(150, 150), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    class_mode=\"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "Let's train our model for 10 more epochs using the same code as previously.\n",
    "\n",
    "Note: make sure you don't re-create the model - we want to continue training the model we already started training.\n",
    "\n",
    "What is the mean of test loss for all the epochs for the model trained with augmentations?\n",
    "\n",
    "* 0.18\n",
    "* 0.48\n",
    "* 0.78\n",
    "* 0.108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "184/184 [==============================] - 14s 76ms/step - loss: 0.4960 - accuracy: 0.7683 - val_loss: 0.4958 - val_accuracy: 0.7691\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 14s 74ms/step - loss: 0.4874 - accuracy: 0.7710 - val_loss: 0.4776 - val_accuracy: 0.7614\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 15s 79ms/step - loss: 0.4844 - accuracy: 0.7822 - val_loss: 0.5204 - val_accuracy: 0.7549\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 15s 82ms/step - loss: 0.4794 - accuracy: 0.7811 - val_loss: 0.4860 - val_accuracy: 0.7723\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 17s 90ms/step - loss: 0.4667 - accuracy: 0.7846 - val_loss: 0.5635 - val_accuracy: 0.7440\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 18s 97ms/step - loss: 0.4698 - accuracy: 0.7868 - val_loss: 0.4704 - val_accuracy: 0.7963\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 17s 91ms/step - loss: 0.4690 - accuracy: 0.7914 - val_loss: 0.4946 - val_accuracy: 0.7669\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 18s 97ms/step - loss: 0.4628 - accuracy: 0.7919 - val_loss: 0.5062 - val_accuracy: 0.7669\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 18s 98ms/step - loss: 0.4557 - accuracy: 0.7941 - val_loss: 0.5501 - val_accuracy: 0.7440\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 19s 101ms/step - loss: 0.4588 - accuracy: 0.7971 - val_loss: 0.5308 - val_accuracy: 0.7484\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5548043847084045"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hist[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.55"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "\n",
    "What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?\n",
    "\n",
    "* 0.38\n",
    "* 0.58\n",
    "* 0.78\n",
    "* 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7516339898109436"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(hist[\"val_accuracy\"][5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
